{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Decision Trees for Classification\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand how decision trees partition the feature space\n",
    "- Visualise decision boundaries without requiring graphviz\n",
    "- Explore the effect of key hyperparameters (max_depth, min_samples_split, min_samples_leaf)\n",
    "- Compare decision trees with KNN and logistic regression\n",
    "- Use cross-validation with different metrics (accuracy, F1, precision, recall)\n",
    "- Understand when different evaluation metrics matter\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Visualising Decision Boundaries\n",
    "\n",
    "One of the key advantages of decision trees is their interpretability. While we can't use graphviz, we can visualize how decision trees partition the feature space by plotting decision boundaries.\n",
    "\n",
    "### 1.1 Helper Function for Plotting Decision Boundaries\n",
    "\n",
    "This function will help us visualize how different models divide the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\", ax=None):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a classification model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: fitted classifier\n",
    "    - X: feature array (n_samples, 2)\n",
    "    - y: labels\n",
    "    - title: plot title\n",
    "    - ax: matplotlib axis (optional)\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Create a mesh to plot decision boundary\n",
    "    h = 0.02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict for each point in the mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n",
    "                        edgecolors='black', s=50)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(scatter, ax=ax)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Decision Trees on Make_Moons Dataset\n",
    "\n",
    "Let's start with a non-linearly separable dataset to see how decision trees handle complex boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate make_moons dataset\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "\n",
    "# Train a decision tree\n",
    "tree_moons = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree_moons.fit(X_moons, y_moons)\n",
    "\n",
    "# Visualize\n",
    "plot_decision_boundary(tree_moons, X_moons, y_moons, \n",
    "                      title=\"Decision Tree on Make_Moons (max_depth=5)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training Accuracy: {tree_moons.score(X_moons, y_moons):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Notice how the decision boundary consists of rectangular regions. Why do you think this is?\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Decision Trees on Make_Blobs with 4 Classes\n",
    "\n",
    "Now let's try a multi-class problem to see how decision trees handle more complex classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate make_blobs with 4 classes\n",
    "X_blobs, y_blobs = make_blobs(n_samples=400, centers=4, n_features=2, \n",
    "                              cluster_std=1.5, random_state=42)\n",
    "\n",
    "# Train a decision tree\n",
    "tree_blobs = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree_blobs.fit(X_blobs, y_blobs)\n",
    "\n",
    "# Visualize\n",
    "plot_decision_boundary(tree_blobs, X_blobs, y_blobs, \n",
    "                      title=\"Decision Tree on Make_Blobs (4 classes, max_depth=5)\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training Accuracy: {tree_blobs.score(X_blobs, y_blobs):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Exercise: Your Turn!\n",
    "\n",
    "Create a decision tree for the make_moons dataset with `max_depth=2` and visualize the decision boundary. How does it compare to the tree with `max_depth=5`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "tree_shallow = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Decision Tree Hyperparameters\n",
    "\n",
    "Decision trees have several important hyperparameters that control their complexity and prevent overfitting:\n",
    "\n",
    "- **max_depth**: Maximum depth of the tree\n",
    "- **min_samples_split**: Minimum samples required to split an internal node\n",
    "- **min_samples_leaf**: Minimum samples required at a leaf node\n",
    "\n",
    "### 2.1 Effect of max_depth\n",
    "\n",
    "Let's see how varying `max_depth` affects the model's complexity and decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for proper evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_moons, y_moons, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "# Try different max_depth values\n",
    "depths = [1, 2, 5, 10, 20]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for i, depth in enumerate(depths):\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    train_score = tree.score(X_train, y_train)\n",
    "    test_score = tree.score(X_test, y_test)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    \n",
    "    plot_decision_boundary(tree, X_train, y_train, \n",
    "                          title=f\"max_depth={depth}\\nTrain: {train_score:.3f}, Test: {test_score:.3f}\",\n",
    "                          ax=axes[i])\n",
    "\n",
    "# Plot train vs test scores\n",
    "axes[-1].plot(depths, train_scores, marker='o', label='Train Score', linewidth=2)\n",
    "axes[-1].plot(depths, test_scores, marker='s', label='Test Score', linewidth=2)\n",
    "axes[-1].set_xlabel('max_depth')\n",
    "axes[-1].set_ylabel('Accuracy')\n",
    "axes[-1].set_title('Train vs Test Accuracy')\n",
    "axes[-1].legend()\n",
    "axes[-1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: At what depth does the model start to overfit? How can you tell?\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Exercise: Exploring min_samples_split\n",
    "\n",
    "The `min_samples_split` parameter controls how many samples are required to split an internal node. Higher values prevent the tree from learning very specific patterns.\n",
    "\n",
    "Complete the code below to train trees with different `min_samples_split` values and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Try min_samples_split values: [2, 10, 30, 50]\n",
    "min_splits = [2, 10, 30, 50]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, min_split in enumerate(min_splits):\n",
    "    # Create and train a decision tree with the current min_samples_split\n",
    "    # Use max_depth=10 to allow the tree to grow\n",
    "    \n",
    "    \n",
    "    # Calculate train and test scores\n",
    "    \n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plot_decision_boundary(tree, X_train, y_train,\n",
    "                          title=f\"min_samples_split={min_split}\\nTrain: {train_score:.3f}, Test: {test_score:.3f}\",\n",
    "                          ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Exercise: Exploring min_samples_leaf\n",
    "\n",
    "Similarly, `min_samples_leaf` specifies the minimum number of samples required at each leaf node. This also helps prevent overfitting.\n",
    "\n",
    "Train trees with `min_samples_leaf` values of [1, 5, 15, 30] and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "min_leafs = [1, 5, 15, 30]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Importance (Alternative to Tree Visualization)\n",
    "\n",
    "Since we can't use graphviz, another way to interpret decision trees is through feature importance. This tells us which features the tree relies on most for making decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a tree on the blobs dataset\n",
    "tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree.fit(X_blobs, y_blobs)\n",
    "\n",
    "# Get feature importances\n",
    "importances = tree.feature_importances_\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(['Feature 1', 'Feature 2'], importances)\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance in Decision Tree')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Feature 1 importance: {importances[0]:.3f}\")\n",
    "print(f\"Feature 2 importance: {importances[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.1: Why Decision Trees Don't Need Feature Scaling\n",
    "\n",
    "Unlike KNN and Logistic Regression, decision trees are **scale-invariant**. They only care about the relative ordering of values, not their magnitude. Let's demonstrate this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with features on very different scales\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Generate data where features have wildly different scales\n",
    "np.random.seed(42)\n",
    "X_unscaled = np.column_stack([\n",
    "    np.random.normal(0, 1, 300),      # Feature 1: mean=0, std=1\n",
    "    np.random.normal(0, 1000, 300)    # Feature 2: mean=0, std=1000 (1000x larger!)\n",
    "])\n",
    "\n",
    "# Create target based on both features\n",
    "y_scale = ((X_unscaled[:, 0] > 0) & (X_unscaled[:, 1] > 0)).astype(int)\n",
    "\n",
    "# Split the data\n",
    "X_train_scale, X_test_scale, y_train_scale, y_test_scale = train_test_split(\n",
    "    X_unscaled, y_scale, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Feature scales (training set):\")\n",
    "print(f\"  Feature 1 - Mean: {X_train_scale[:, 0].mean():.2f}, Std: {X_train_scale[:, 0].std():.2f}\")\n",
    "print(f\"  Feature 2 - Mean: {X_train_scale[:, 1].mean():.2f}, Std: {X_train_scale[:, 1].std():.2f}\")\n",
    "print(f\"\\n  Feature 2 is ~1000x larger in scale!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this code will scale the datamake a knn model, a logistic regression model and a decision tree model with the scaled data, and also models with the unscaled data\n",
    "\n",
    "We should be able to see how scaling makes no difference to the Decision Tree while large differences to the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_scale)\n",
    "X_test_scaled = scaler.transform(X_test_scale)\n",
    "\n",
    "# Train models on UNSCALED data\n",
    "tree_unscaled = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
    "logreg_unscaled = LogisticRegression(random_state=42)\n",
    "\n",
    "tree_unscaled.fit(X_train_scale, y_train_scale)\n",
    "knn_unscaled.fit(X_train_scale, y_train_scale)\n",
    "logreg_unscaled.fit(X_train_scale, y_train_scale)\n",
    "\n",
    "# Train models on SCALED data\n",
    "tree_scaled = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "logreg_scaled = LogisticRegression(random_state=42)\n",
    "\n",
    "tree_scaled.fit(X_train_scaled, y_train_scale)\n",
    "knn_scaled.fit(X_train_scaled, y_train_scale)\n",
    "logreg_scaled.fit(X_train_scaled, y_train_scale)\n",
    "\n",
    "# Compare performance\n",
    "results_scaling = pd.DataFrame({\n",
    "    'Decision Tree (Unscaled)': [tree_unscaled.score(X_test_scale, y_test_scale)],\n",
    "    'Decision Tree (Scaled)': [tree_scaled.score(X_test_scaled, y_test_scale)],\n",
    "    'KNN (Unscaled)': [knn_unscaled.score(X_test_scale, y_test_scale)],\n",
    "    'KNN (Scaled)': [knn_scaled.score(X_test_scaled, y_test_scale)],\n",
    "    'Logistic Reg (Unscaled)': [logreg_unscaled.score(X_test_scale, y_test_scale)],\n",
    "    'Logistic Reg (Scaled)': [logreg_scaled.score(X_test_scaled, y_test_scale)]\n",
    "}, index=['Accuracy'])\n",
    "\n",
    "print(\"\\nTest Accuracy Comparison:\")\n",
    "print(results_scaling.T.round(3))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "models = ['Decision Tree', 'KNN', 'Logistic Reg']\n",
    "unscaled_scores = [\n",
    "    tree_unscaled.score(X_test_scale, y_test_scale),\n",
    "    knn_unscaled.score(X_test_scale, y_test_scale),\n",
    "    logreg_unscaled.score(X_test_scale, y_test_scale)\n",
    "]\n",
    "scaled_scores = [\n",
    "    tree_scaled.score(X_test_scaled, y_test_scale),\n",
    "    knn_scaled.score(X_test_scaled, y_test_scale),\n",
    "    logreg_scaled.score(X_test_scaled, y_test_scale)\n",
    "]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, unscaled_scores, width, label='Unscaled', alpha=0.8)\n",
    "axes[0].bar(x + width/2, scaled_scores, width, label='Scaled', alpha=0.8)\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_title('Impact of Feature Scaling on Model Performance')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Show the difference\n",
    "differences = np.array(scaled_scores) - np.array(unscaled_scores)\n",
    "colors = ['green' if d > 0.01 else 'red' if d < -0.01 else 'gray' for d in differences]\n",
    "axes[1].bar(models, differences, color=colors, alpha=0.7)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1].set_ylabel('Accuracy Difference (Scaled - Unscaled)')\n",
    "axes[1].set_title('Performance Change from Scaling')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Decision Tree: Scaling makes NO difference (same accuracy)\")\n",
    "print(\"KNN & Logistic Regression: Scaling makes a HUGE difference!\")\n",
    "print(\"\\nWhy? Decision trees only care about 'Is X > threshold?', not the\")\n",
    "print(\"actual magnitude of X. KNN uses distances and LogReg uses magnitudes,\")\n",
    "print(\"so they're sensitive to scale.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Comparing Classifiers on a Real Dataset\n",
    "\n",
    "Now let's work with a real-world dataset where class imbalance makes the choice of evaluation metric crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Loading the Credit Card Fraud Dataset\n",
    "\n",
    "We'll use the famous Credit Card Fraud Detection dataset from Kaggle. This is a classic example where:\n",
    "- Classes are **highly imbalanced** (only ~0.17% fraud!)\n",
    "- **Accuracy can be misleading** (a model that always predicts \"not fraud\" would have >99.8% accuracy)\n",
    "- **Precision and recall matter**: Missing fraud (low recall) costs money, but too many false alarms (low precision) wastes resources\n",
    "\n",
    "#### How to get the dataset:\n",
    "1. Download `creditcard.csv` from: https://www.kaggle.com/mlg-ulb/creditcardfraud or Moodle\n",
    "2. Place it in the same directory as this notebook\n",
    "3. Run the cell below (it will automatically fall back to simulated data if file not found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the real Kaggle dataset\n",
    "try:\n",
    "    print(\"Attempting to load Kaggle Credit Card Fraud dataset...\")\n",
    "    df = pd.read_csv('creditcard.csv')\n",
    "    \n",
    "    # Drop the Time column as it's not useful for classification\n",
    "    df = df.drop('Time', axis=1)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_fraud = df.drop('Class', axis=1).values\n",
    "    y_fraud = df['Class'].values\n",
    "    \n",
    "    print(f\"✓ Successfully loaded REAL dataset!\")\n",
    "    print(f\"  Total transactions: {len(y_fraud):,}\")\n",
    "    print(f\"  Features: {X_fraud.shape[1]}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"⚠ Kaggle dataset not found. Using simulated data instead.\")\n",
    "    print(\"  To use real data, download from: https://www.kaggle.com/mlg-ulb/creditcardfraud\\n\")\n",
    "    \n",
    "    from sklearn.datasets import make_classification\n",
    "    \n",
    "    # Create a realistic simulation with extreme imbalance\n",
    "    X_fraud, y_fraud = make_classification(\n",
    "        n_samples=10000,\n",
    "        n_features=29,\n",
    "        n_informative=20,\n",
    "        n_redundant=5,\n",
    "        n_classes=2,\n",
    "        weights=[0.998, 0.002],  # 99.8% non-fraud, 0.2% fraud (similar to real data)\n",
    "        flip_y=0.01,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"  Generated simulated dataset\")\n",
    "\n",
    "# Check class distribution\n",
    "unique, counts = np.unique(y_fraud, return_counts=True)\n",
    "print(\"\\nClass distribution:\")\n",
    "for label, count in zip(unique, counts):\n",
    "    fraud_label = \"Non-Fraud\" if label == 0 else \"Fraud\"\n",
    "    print(f\"  {fraud_label} (Class {label}): {count:,} ({count/len(y_fraud)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n⚠ Imbalance ratio: {counts[0]/counts[1]:.1f}:1\")\n",
    "print(f\"\\nThis means a 'always predict non-fraud' model would get {counts[0]/len(y_fraud)*100:.2f}% accuracy!\")\n",
    "\n",
    "# Split the data\n",
    "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(\n",
    "    X_fraud, y_fraud, test_size=0.3, random_state=42, stratify=y_fraud\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train_fraud):,} samples\")\n",
    "print(f\"Test set: {len(X_test_fraud):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Baseline: Accuracy with Three Classifiers\n",
    "\n",
    "Let's first compare Decision Trees, KNN, and Logistic Regression using simple accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train three classifiers\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_fraud, y_train_fraud)\n",
    "    y_pred = model.predict(X_test_fraud)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_fraud, y_pred)\n",
    "    precision = precision_score(y_test_fraud, y_pred)\n",
    "    recall = recall_score(y_test_fraud, y_pred)\n",
    "    f1 = f1_score(y_test_fraud, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df.round(3))\n",
    "\n",
    "# Visualize\n",
    "results_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Model Comparison on Imbalanced Dataset')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Which model has the highest accuracy? Does this mean it's the best model for fraud detection? Why or why not?\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Understanding the Metrics\n",
    "\n",
    "Let's look at the confusion matrices to understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    y_pred = model.predict(X_test_fraud)\n",
    "    cm = confusion_matrix(y_test_fraud, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    im = axes[i].imshow(cm, cmap='Blues')\n",
    "    axes[i].set_title(f'{name}\\nConfusion Matrix')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "    axes[i].set_xticks([0, 1])\n",
    "    axes[i].set_yticks([0, 1])\n",
    "    axes[i].set_xticklabels(['Non-Fraud', 'Fraud'])\n",
    "    axes[i].set_yticklabels(['Non-Fraud', 'Fraud'])\n",
    "    \n",
    "    # Add text annotations\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            text = axes[i].text(k, j, cm[j, k],\n",
    "                              ha=\"center\", va=\"center\", color=\"black\", fontsize=14)\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Look at the bottom-right cell (actual fraud, predicted fraud). Which model catches the most fraud cases?\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Exercise: Cross-Validation with Different Metrics\n",
    "\n",
    "Now let's use cross-validation to get more robust performance estimates. We'll compare models using both accuracy and F1-score.\n",
    "\n",
    "Complete the code below to perform 5-fold cross-validation with both metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Perform 5-fold cross-validation for each model\n",
    "# Calculate both accuracy and F1 scores\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Calculate cross-validation scores for accuracy\n",
    "    accuracy_scores = cross_val_score(model, X_fraud, y_fraud, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Calculate cross-validation scores for F1\n",
    "    # Hint: use scoring='f1'\n",
    "    \n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'Accuracy (mean)': accuracy_scores.mean(),\n",
    "        'Accuracy (std)': accuracy_scores.std(),\n",
    "        'F1 (mean)': f1_scores.mean(),\n",
    "        'F1 (std)': f1_scores.std()\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "cv_df = pd.DataFrame(cv_results).T\n",
    "print(\"\\nCross-Validation Results (5-fold):\")\n",
    "print(cv_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Exercise: GridSearchCV for Hyperparameter Tuning\n",
    "\n",
    "Let's use GridSearchCV to find the best hyperparameters for our Decision Tree, optimizing for F1-score instead of accuracy.\n",
    "\n",
    "Complete the code below to tune the Decision Tree hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Define parameter grid for Decision Tree\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "# Use scoring='f1' and cv=5\n",
    "tree_base = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "\n",
    "# Fit the grid search\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"Best parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\nBest F1 score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Compare with default parameters\n",
    "default_tree = DecisionTreeClassifier(random_state=42)\n",
    "default_f1 = cross_val_score(default_tree, X_fraud, y_fraud, cv=5, scoring='f1').mean()\n",
    "print(f\"Default tree F1 score: {default_f1:.3f}\")\n",
    "print(f\"Improvement: {(grid_search.best_score_ - default_f1):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Final Comparison: Tuned Models\n",
    "\n",
    "Now let's compare our tuned Decision Tree with tuned versions of KNN and Logistic Regression, all optimized for F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for all models\n",
    "param_grids = {\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'min_samples_split': [2, 10, 20],\n",
    "        'min_samples_leaf': [1, 5, 10]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs']\n",
    "    }\n",
    "}\n",
    "\n",
    "base_models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "tuned_models = {}\n",
    "tuning_results = {}\n",
    "\n",
    "for name in base_models.keys():\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        base_models[name],\n",
    "        param_grids[name],\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train_fraud, y_train_fraud)\n",
    "    \n",
    "    tuned_models[name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Test on held-out test set\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test_fraud)\n",
    "    \n",
    "    tuning_results[name] = {\n",
    "        'Best Params': grid_search.best_params_,\n",
    "        'CV F1 Score': grid_search.best_score_,\n",
    "        'Test Accuracy': accuracy_score(y_test_fraud, y_pred),\n",
    "        'Test Precision': precision_score(y_test_fraud, y_pred),\n",
    "        'Test Recall': recall_score(y_test_fraud, y_pred),\n",
    "        'Test F1': f1_score(y_test_fraud, y_pred)\n",
    "    }\n",
    "    \n",
    "    print(f\"  Best params: {grid_search.best_params_}\")\n",
    "    print(f\"  CV F1: {grid_search.best_score_:.3f}\")\n",
    "    print(f\"  Test F1: {f1_score(y_test_fraud, y_pred):.3f}\")\n",
    "\n",
    "# Create summary DataFrame (excluding best params for display)\n",
    "summary_data = {name: {k: v for k, v in results.items() if k != 'Best Params'} \n",
    "                for name, results in tuning_results.items()}\n",
    "summary_df = pd.DataFrame(summary_data).T\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS: Tuned Models on Test Set\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.round(3))\n",
    "\n",
    "# Visualize final comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot metrics comparison\n",
    "summary_df[['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1']].plot(\n",
    "    kind='bar', ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('Tuned Models: Test Set Performance')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45)\n",
    "\n",
    "# Plot CV vs Test F1\n",
    "x = np.arange(len(tuned_models))\n",
    "width = 0.35\n",
    "axes[1].bar(x - width/2, summary_df['CV F1 Score'], width, label='CV F1', alpha=0.8)\n",
    "axes[1].bar(x + width/2, summary_df['Test F1'], width, label='Test F1', alpha=0.8)\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('Cross-Validation vs Test F1 Scores')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(tuned_models.keys(), rotation=45)\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Exercise: Comparing Accuracy vs F1 Optimization\n",
    "\n",
    "Let's see what happens when we optimize for accuracy instead of F1-score. Complete the code below to tune a Decision Tree using accuracy as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Tune a Decision Tree using accuracy instead of F1\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV with scoring='accuracy'\n",
    "\n",
    "\n",
    "# Fit and evaluate\n",
    "\n",
    "\n",
    "# Compare with F1-optimized tree\n",
    "y_pred_acc = grid_search_acc.predict(X_test_fraud)\n",
    "y_pred_f1 = tuned_models['Decision Tree'].predict(X_test_fraud)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Optimized for Accuracy': {\n",
    "        'Test Accuracy': accuracy_score(y_test_fraud, y_pred_acc),\n",
    "        'Test Precision': precision_score(y_test_fraud, y_pred_acc),\n",
    "        'Test Recall': recall_score(y_test_fraud, y_pred_acc),\n",
    "        'Test F1': f1_score(y_test_fraud, y_pred_acc)\n",
    "    },\n",
    "    'Optimized for F1': {\n",
    "        'Test Accuracy': accuracy_score(y_test_fraud, y_pred_f1),\n",
    "        'Test Precision': precision_score(y_test_fraud, y_pred_f1),\n",
    "        'Test Recall': recall_score(y_test_fraud, y_pred_f1),\n",
    "        'Test F1': f1_score(y_test_fraud, y_pred_f1)\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"\\nComparison: Accuracy vs F1 Optimization\")\n",
    "print(comparison.T.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why does optimizing for F1 produce different results than optimizing for accuracy? Which optimization strategy would you choose for fraud detection and why?\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Summary and Reflection\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Decision Tree Interpretability**: Decision trees create rectangular decision boundaries by making axis-parallel splits. We can visualize these boundaries and understand feature importance even without tree diagrams.\n",
    "\n",
    "2. **Hyperparameter Effects**:\n",
    "   - `max_depth` controls model complexity and overfitting\n",
    "   - `min_samples_split` and `min_samples_leaf` provide additional regularization\n",
    "   - Proper tuning is essential for good generalization\n",
    "\n",
    "3. **Metric Selection Matters**:\n",
    "   - Accuracy can be misleading on imbalanced datasets\n",
    "   - F1-score balances precision and recall\n",
    "   - The right metric depends on the cost of false positives vs false negatives\n",
    "\n",
    "4. **Model Comparison**:\n",
    "   - Decision Trees, KNN, and Logistic Regression each have strengths\n",
    "   - Cross-validation provides robust performance estimates\n",
    "   - GridSearchCV helps find optimal hyperparameters\n",
    "\n",
    "### Reflection Questions:\n",
    "\n",
    "1. When would you prefer a Decision Tree over KNN or Logistic Regression?\n",
    "2. How do you decide which evaluation metric to use for a given problem?\n",
    "3. What are the trade-offs between model complexity and interpretability?\n",
    "\n",
    "---\n",
    "\n",
    "## Bonus Challenge (Optional)\n",
    "\n",
    "Try the following extensions:\n",
    "\n",
    "1. **Experiment with class weights**: Decision trees support a `class_weight='balanced'` parameter. How does this affect performance on imbalanced data?\n",
    "\n",
    "2. **Feature engineering**: Create new features from the fraud dataset and see if they improve model performance.\n",
    "\n",
    "3. **Ensemble methods**: Research and try `RandomForestClassifier` - how does it compare to a single decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your bonus code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
